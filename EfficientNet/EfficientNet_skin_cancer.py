# -*- coding: utf-8 -*-
"""EfficientNet_Skin_Cancer_Dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hPeu6irykPCHJi8BV-l0UPeRdAz_7yDv
"""

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.utils.class_weight import compute_class_weight
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
from tensorflow.keras.models import Model

df = pd.read_csv("/content/drive/MyDrive/Skin_Cancer_MNIST_Dataset/HAM10000_metadata.csv")   # Load CSV
df['image_path'] = df['image_id'].apply(lambda x: f"/content/drive/MyDrive/Skin_Cancer_MNIST_Dataset/HAM10000_images/{x}.jpg")   # Add full image path
df = df.rename(columns={'dx': 'label'})   # Rename label column for clarity
print(df['label'].value_counts())   # Check class distribution

# Encode labels
le = LabelEncoder()
df['label_encoded'] = le.fit_transform(df['label'])

# Train-test split
train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['label_encoded'], random_state=42)

print("Train size:", len(train_df))
print("Test size:", len(test_df))
print("Classes:", le.classes_)

# Step 2: Compute Class Weights
class_weights = compute_class_weight(
    class_weight='balanced',
    classes=np.unique(train_df['label_encoded'].astype(int)),
    y=train_df['label_encoded'].astype(int)
)
class_weights = dict(enumerate(class_weights))

img_size = (224, 224)
batch_size = 32

train_gen = ImageDataGenerator(rescale=1./255, horizontal_flip=True, zoom_range=0.2)
test_gen = ImageDataGenerator(rescale=1./255)

#  Convert numeric labels to strings for compatibility
train_df['label_encoded'] = train_df['label_encoded'].astype(str)
test_df['label_encoded'] = test_df['label_encoded'].astype(str)

train_data = train_gen.flow_from_dataframe(
    train_df,
    x_col='image_path',
    y_col='label_encoded',
    target_size=img_size,
    class_mode='sparse',
    batch_size=batch_size
)

test_data = test_gen.flow_from_dataframe(
    test_df,
    x_col='image_path',
    y_col='label_encoded',
    target_size=img_size,
    class_mode='sparse',
    batch_size=batch_size,
    shuffle=False
)

base_model = EfficientNetB0(include_top=False, weights='imagenet', input_shape=(224, 224, 3))
base_model.trainable = False  # Freeze base

x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dropout(0.3)(x)
output = Dense(len(le.classes_), activation='softmax')(x)

model = Model(inputs=base_model.input, outputs=output)

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

history = model.fit(
    train_data,
    validation_data=test_data,
    epochs=5
)

test_loss, test_accuracy = model.evaluate(test_data)
print(f"‚úÖ Final Test Accuracy: {test_accuracy:.4f}")

from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support
import numpy as np

# Get true & predicted labels
y_true = test_data.classes  # ground truth labels
y_pred_probs = model.predict(test_data)
y_pred = np.argmax(y_pred_probs, axis=1)  # predicted class indices

# Decode labels back to class names (optional)
class_labels = list(test_data.class_indices.keys())

# Classification report
print("üìã Classification Report:")
print(classification_report(y_true, y_pred, target_names=class_labels))

cm = confusion_matrix(y_true, y_pred)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=class_labels,
            yticklabels=class_labels)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix (Test Data)")
plt.show()

"""For Better results, we will now fine tune the model and again train the model by unfreezing EfficientNet's Deeper layers."""

# Unfreeze the base model
base_model.trainable = True

# Recompiling the Model by lowering the learning Rate.

from tensorflow.keras.optimizers import Adam

model.compile(
    optimizer=Adam(learning_rate=1e-5),  # üîΩ Much lower LR for fine-tuning
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# training again the model.
fine_tune_epochs = 5

history_finetune = model.fit(
    train_data,
    validation_data=test_data,
    epochs=fine_tune_epochs
)

test_loss, test_acc = model.evaluate(test_data)
print(f"üîç Test Accuracy After Fine-Tuning: {test_acc:.4f}")